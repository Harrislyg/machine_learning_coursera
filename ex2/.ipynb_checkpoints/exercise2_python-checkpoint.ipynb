{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafile = 'ex1data1.txt'\n",
    "cols = np.loadtxt(datafile, delimiter=',', usecols=(0,1), unpack=True)\n",
    "print 'X1: ', cols\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "print X\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "\n",
    "m = y.size\n",
    "X = np.insert(X, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print X"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Plot the data to see what it looks like\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(X[:,1],y[:,0],'rx',markersize=10)\n",
    "plt.grid(True) #Always plot.grid true!\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.xlabel('Population of City in 10,000s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## Gradient Descent ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = 1500\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def h(theta, X):\n",
    "    # print np.dot(X, theta)\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "def compute_cost(theta,X,y):\n",
    "    return float((1./(2*m)) * np.dot((h(theta,X)-y).T,(h(theta,X)-y)))\n",
    "\n",
    "initial_theta = np.zeros((X.shape[1],1))\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print compute_cost(initial_theta, X, y)\n",
    "print initial_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.zeros([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Actual gradient descent minimizing routine\n",
    "def descendGradient(X, theta_start = np.zeros(2)):\n",
    "    \"\"\"\n",
    "    theta_start is an n- dimensional vector of initial theta guess\n",
    "    X is matrix with n- columns and m- rows\n",
    "    \"\"\"\n",
    "    theta = theta_start\n",
    "    jvec = [] #Used to plot cost as function of iteration\n",
    "    thetahistory = [] #Used to visualize the minimization path later on\n",
    "    for meaninglessvariable in xrange(iterations):\n",
    "        tmptheta = theta\n",
    "        jvec.append(compute_cost(theta,X,y))\n",
    "        # Buggy line\n",
    "        #thetahistory.append(list(tmptheta))\n",
    "        # Fixed line\n",
    "        thetahistory.append(list(theta[:,0]))\n",
    "        #Simultaneously updating theta values\n",
    "        for j in xrange(len(tmptheta)):\n",
    "            tmptheta[j] = theta[j] - (alpha/m)*np.sum((h(initial_theta,X) - y)*np.array(X[:,j]).reshape(m,1))\n",
    "        theta = tmptheta\n",
    "    return theta, thetahistory, jvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Actually run gradient descent to get the best-fit theta values\n",
    "initial_theta = np.zeros((X.shape[1],1))\n",
    "theta, thetahistory, jvec = descendGradient(X,initial_theta)\n",
    "print theta\n",
    "#Plot the convergence of the cost function\n",
    "def plotConvergence(jvec):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(len(jvec)),jvec,'bo')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Convergence of Cost Function\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Cost function\")\n",
    "    dummy = plt.xlim([-0.05*iterations,1.05*iterations])\n",
    "    #dummy = plt.ylim([4,8])\n",
    "\n",
    "\n",
    "plotConvergence(jvec)\n",
    "dummy = plt.ylim([4,7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros((X.shape[1],1))\n",
    "def GradientDescent(X, y,theta, alpha, num_iters):\n",
    "    for i in range(num_iters):\n",
    "        k = 0\n",
    "        g = 0\n",
    "        h_theta = h(theta, X)\n",
    "        for j in range(m):\n",
    "            k = k + (h_theta[j] - y[j]) * X[j,0]\n",
    "            g = g + (h_theta[j] - y[j]) * X[j,1]\n",
    "        theta[0] = theta[0] - (alpha/m) * k\n",
    "        theta[1] = theta[1] - (alpha/m) * g\n",
    "    \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GradientDescent(X, y, initial_theta, 0.01, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Comment on the logic #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Linear Regression with multiple variables ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'ex1data2.txt'\n",
    "cols = np.loadtxt(datafile,delimiter=',',usecols=(0,1,2),unpack=True)\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "X = np.insert(X,0,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_means = []\n",
    "feature_std = []\n",
    "X_norm = X.copy()\n",
    "for i in range(X.shape[1]):\n",
    "    feature_means.append(np.mean(X_norm[:,i]))\n",
    "    print feature_means\n",
    "    feature_std.append(np.std(X_norm[:,i]))\n",
    "    if not i: continue\n",
    "    X_norm[:,i] = (X_norm[:,i] - feature_means[i]) / feature_std[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Multivariate Gradient Descent ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def h(theta, X):\n",
    "    # print np.dot(X, theta)\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "def compute_cost(theta,X,y,m):\n",
    "    return float((1./(2*m)) * np.dot((h(theta,X)-y).T,(h(theta,X)-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros((Xnorm.shape[1], 1))\n",
    "m = y.size\n",
    "compute_cost(initial_theta, X_norm, y, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradientDescentMulti(X, y, theta, alpha, num_iters,n):\n",
    "    for i in range(num_iters):\n",
    "        theta_new = theta\n",
    "        for j in range(n):\n",
    "            theta_new[j] = theta[j] - ((alpha / m) * np.sum((h(theta, X)-y) * X[:,j]))\n",
    "        theta = theta_new\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradientDescentMulti(X_norm,y, initial_theta, 0.01, 400,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "def normEqn(X,y):\n",
    "    return np.dot(np.dot(inv(np.dot(X.T,X)), X.T), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def h(theta,X): #Linear hypothesis function\n",
    "    return np.dot(X,theta)\n",
    "\n",
    "def computeCost(mytheta,X,y): #Cost function\n",
    "    \"\"\"\n",
    "    theta_start is an n- dimensional vector of initial theta guess\n",
    "    X is matrix with n- columns and m- rows\n",
    "    y is a matrix with m- rows and 1 column\n",
    "    \"\"\"\n",
    "    #note to self: *.shape is (rows, columns)\n",
    "    return float((1./(2*m)) * np.dot((h(mytheta,X)-y).T,(h(mytheta,X)-y)))\n",
    "\n",
    "#Test that running computeCost with 0's as theta returns 32.07:\n",
    "\n",
    "initial_theta = np.zeros((X.shape[1],1)) #(theta is a vector with n rows and 1 columns (if X has n features) )\n",
    "print computeCost(initial_theta,X,y)\n",
    "\n",
    "#Actual gradient descent minimizing routine\n",
    "def descendGradient(X, theta_start = np.zeros(2)):\n",
    "    \"\"\"\n",
    "    theta_start is an n- dimensional vector of initial theta guess\n",
    "    X is matrix with n- columns and m- rows\n",
    "    \"\"\"\n",
    "    theta = theta_start\n",
    "    jvec = [] #Used to plot cost as function of iteration\n",
    "    thetahistory = [] #Used to visualize the minimization path later on\n",
    "    for meaninglessvariable in xrange(iterations):\n",
    "        tmptheta = theta\n",
    "        jvec.append(computeCost(theta,X,y))\n",
    "        # Buggy line\n",
    "        #thetahistory.append(list(tmptheta))\n",
    "        # Fixed line\n",
    "        thetahistory.append(list(theta[:,0]))\n",
    "        #Simultaneously updating theta values\n",
    "        for j in xrange(len(tmptheta)):\n",
    "            tmptheta[j] = theta[j] - (alpha/m)*np.sum((h(initial_theta,X) - y)*np.array(X[:,j]).reshape(m,1))\n",
    "        theta = tmptheta\n",
    "    return theta, thetahistory, jvec\n",
    "\n",
    "datafile = 'ex1data2.txt'\n",
    "#Read into the data file\n",
    "cols = np.loadtxt(datafile,delimiter=',',usecols=(0,1,2),unpack=True) #Read in comma separated data\n",
    "#Form the usual \"X\" matrix and \"y\" vector\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "m = y.size # number of training examples\n",
    "#Insert the usual column of 1's into the \"X\" matrix\n",
    "X = np.insert(X,0,1,axis=1)\n",
    "\n",
    "#Feature normalizing the columns (subtract mean, divide by standard deviation)\n",
    "#Store the mean and std for later use\n",
    "#Note don't modify the original X matrix, use a copy\n",
    "stored_feature_means, stored_feature_stds = [], []\n",
    "Xnorm = X.copy()\n",
    "for icol in xrange(Xnorm.shape[1]):\n",
    "    stored_feature_means.append(np.mean(Xnorm[:,icol]))\n",
    "    stored_feature_stds.append(np.std(Xnorm[:,icol]))\n",
    "    #Skip the first column\n",
    "    if not icol: continue\n",
    "    #Faster to not recompute the mean and std again, just used stored values\n",
    "    Xnorm[:,icol] = (Xnorm[:,icol] - stored_feature_means[-1])/stored_feature_stds[-1]\n",
    "\n",
    "#Run gradient descent with multiple variables, initial theta still set to zeros\n",
    "#(Note! This doesn't work unless we feature normalize! \"overflow encountered in multiply\")\n",
    "initial_theta = np.zeros((Xnorm.shape[1],1))\n",
    "theta, thetahistory, jvec = descendGradient(Xnorm,initial_theta)\n",
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
